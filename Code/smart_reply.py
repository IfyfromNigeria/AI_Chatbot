# -*- coding: utf-8 -*-
"""Smart_Reply.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XarTR26_O-gxY1Rvgpso714XwMwLHUcw

**SMART REPLY: AI POWERED CHATBOT FOR CONTEXT-AWARE CUSTOMER REVIEW RESPONSES**

# PROJECT SETUP
"""

#PROJECT SETUP: Mount Google Drive & Install Dependencies ====
from google.colab import drive
drive.mount('/content/drive')

# Define your project root folder
project_dir = "/content/drive/MyDrive/T5_Chatbot_Project"

# Create folders if they don't exist
import os
folders = ["datasets", "models", "outputs", "logs"]
for folder in folders:
    os.makedirs(os.path.join(project_dir, folder), exist_ok=True)

print("Project subfolders created successfully!")

# Install Transformers and Datasets
!pip install -q transformers datasets evaluate nltk gradio

!pip install datasets

"""# DATASETS

LOADING DATASETS FOR THE PROJECT
1. Sentiment Analysis- Amazon Reviews 2023 dataset. (Hugging Face)
2. Emotion Detection- GoEmotions dataset by Google Research (Hugging face)
3. Customer Chat Interaction- DailyDialog dataset(Hugging Face) & Customer Support on Twitter Dataset (Kaggle)

**Sentiment Analysis (Amazon Reviews 2023)**: The dataset was too large, so I chose a six categories from the list of categories available: Baby products, Beauty, fashion, health, software, video games and 5000 reviews each.
"""

# Choose one category at a time
category = "Video_games"
url = "https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/review_categories/Video_Games.jsonl.gz"
save_path = f"/content/drive/MyDrive/T5_Chatbot_Project/datasets/{category}_sample.csv"
sample_limit = 5000

# Stream and sample
print(f"Streaming and sampling {sample_limit} from {category}...")
response = requests.get(url, stream=True)
compressed_file = BytesIO(response.content)

samples = []
with gzip.open(compressed_file, 'rt', encoding='utf-8') as f:
    for i, line in enumerate(f):
        if i >= sample_limit:
            break
        try:
            samples.append(json.loads(line))
        except json.JSONDecodeError:
            continue

df = pd.DataFrame(samples)

# Save only necessary columns
if 'text' in df.columns and 'rating' in df.columns:
    df = df[['text', 'rating']].dropna()
    df.to_csv(save_path, index=False)
    print(f"Saved {len(df)} reviews to {save_path}")
else:
    print(f"Required columns not found. Columns available: {df.columns.tolist()}")

# Clean up memory
# del df, samples, compressed_file
# gc.collect()

# Combining the datasets to a full dataframe
datasets_path = "/content/drive/MyDrive/T5_Chatbot_Project/datasets"
categories = ["Fashion", "Beauty", "Baby_products", "Software", "Video_games", "Health_personal_care"]

# Load and store all DataFrames
df_list = []
for cat in categories:
    file_path = os.path.join(datasets_path, f"{cat}.csv")
    df = pd.read_csv(file_path)
    df['category'] = cat  # Add a column to indicate the source category
    df_list.append(df)

# Combine all DataFrames
full_df = pd.concat(df_list, ignore_index=True)
print(f"Combined dataset shape: {full_df.shape}")
full_df.head()

# Mapping star ratings to numeric
def map_rating(rating):
    if rating <= 2:
        return "negative"
    elif rating == 3:
        return "neutral"
    else:
        return "positive"

full_df['sentiment'] = full_df['rating'].apply(map_rating)
print(full_df['sentiment'].value_counts())

# Saving full dataset to drive
combined_path = os.path.join(datasets_path, "combined_sentiment_dataset.csv")
full_df.to_csv(combined_path, index=False)
print(f"Saved combined dataset to {combined_path}")

"""**Emotion Detection Dataset (GoEmotions)**"""

!pip install datasets

from datasets import load_dataset

# Loading the simplified config (single-label)
goemotions = load_dataset("google-research-datasets/go_emotions", "simplified")
goemotions

from datasets import concatenate_datasets
combined = concatenate_datasets([
    goemotions["train"],
    goemotions["validation"],
    goemotions["test"]
])
labels = combined.features["labels"].feature.names
len(labels)

# Map labels to label names
def map_label_names(example):
    example["labels"] = labels[example["labels"][0]]  # pick first label if its a list
    return example

# Apply mapping to all examples
labelled_ds = combined.map(map_label_names)

# Convert to pandas dataframe with text and label_name columns
goemotions_df = labelled_ds.to_pandas()[["text", "labels"]]
goemotions_df.rename(columns={"label_name": "emotion"}, inplace=True)

print(goemotions_df.head())

# Saving GoEmotions dataset to drive
combined_path = os.path.join(datasets_path, "goemotions.csv")
goemotions_df.to_csv(combined_path, index=False)
print(f"Saved GoEmotions dataset to {combined_path}")

goemotions_df.shape

"""Customer Chat Interaction (DailyDialog & Customer Support Dataset)

DAILYDIALOG DATASET FROM HUGGING FACE
"""

dailydialog = load_dataset("daily_dialog")
print(dailydialog)
dailydialog["train"][0]

# Combining the train, test and validation
dailydialog_df = pd.concat([
    dailydialog["train"].to_pandas(),
    dailydialog["test"].to_pandas(),
    dailydialog["validation"].to_pandas()
])

act_label =['dummy', 'inform', 'question', 'directive', 'commissive']
emotion_label = ['no emotion', 'anger', 'disgust', 'fear', 'happiness', 'sadness', 'surprise']
dailydialog_df = dailydialog_df.explode(["dialog", "emotion", "act"])

dailydialog_df['act_label'] = dailydialog_df['act'].apply(lambda x: act_label[x])
dailydialog_df['emotion_label'] = dailydialog_df['emotion'].apply(lambda x: emotion_label[x])

dailydialog_df.head(10)

# Saving DailyDialog Dataset to Drive
combined_path = os.path.join(datasets_path, "dailydialog.csv")
dailydialog_df.to_csv(combined_path, index=False)
print(f"Saved DailyDialog dataset to {combined_path}")

dailydialog_df.shape

"""CUSTOMER SUPPORT ON TWITTER DATASET"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("thoughtvector/customer-support-on-twitter")

print("Path to dataset files:", path)

import zipfile

# Move kaggle.json to the right directory
os.makedirs("/root/.kaggle", exist_ok=True)
os.rename("kaggle.json", "/root/.kaggle/kaggle.json")

# Set permissions
os.chmod("/root/.kaggle/kaggle.json", 600)

# Download the dataset
!kaggle datasets download -d thoughtvector/customer-support-on-twitter

# Unzip it
with zipfile.ZipFile("customer-support-on-twitter.zip", "r") as zip_ref:
    zip_ref.extractall("/content/drive/MyDrive/T5_Chatbot_Project/datasets/")

# support_df = pd.read_csv("/content/drive/MyDrive/T5_Chatbot_Project/datasets/twcs.csv")
support_df.head()

customers = support_df[~support_df["author_id"].str.contains("support", case=False, na=False)]
support = support_df[support_df["author_id"].str.contains("support", case=False, na=False)]

# Step 2: Merge based on reply relationship
merged = pd.merge(
    customers,
    support,
    how="inner",
    left_on="tweet_id",
    right_on="in_response_to_tweet_id",
    suffixes=("_customer", "_support")
)

# Step 3: Select and clean up columns
chat_df = merged[["text_customer", "text_support"]].rename(columns={
    "text_customer": "question",
    "text_support": "response"
})

# Drop any rows with missing text
chat_df.dropna(inplace=True)

chat_df.head()

# Saving Customer Support on Twitter Dataset to Drive
combined_path = os.path.join(datasets_path, "cust_support.csv")
chat_df.to_csv(combined_path, index=False)
print(f"Saved Customer Support dataset to {combined_path}")

"""# DATA PREPROCESSING & PREPARATION"""

import re

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r"http\S+", "", text)  # remove URLs
    text = re.sub(r"<.*?>", "", text)    # remove HTML tags
    text = re.sub(r"[^a-zA-Z0-9\s.,!?]", "", text)  # remove special chars [^a-zA-Z0-9\s.,!?]
    text = re.sub(r"\s+", " ", text).strip()  # normalize spaces
    return text

# LOADING THE DATASETS
sentiment = pd.read_csv("/content/combined_sentiment_dataset.csv")
emotion = pd.read_csv("/content/goemotions.csv")
dailydialog = pd.read_csv("/content/dailydialog.csv")
support = pd.read_csv("/content/cust_support.csv")

sentiment['text'] = sentiment['text'].apply(clean_text)
emotion['text'] = emotion['text'].apply(clean_text)
dailydialog['dialog'] = dailydialog['dialog'].apply(clean_text)
support['question'] = support['question'].apply(clean_text)
support['response'] = support['response'].apply(clean_text)

sentiment['text'] = sentiment['text'].apply(clean_text)
emotion['text'] = emotion['text'].apply(clean_text)
dailydialog['dialog'] = dailydialog['dialog'].apply(clean_text)
support['question'] = support['question'].apply(clean_text)
support['response'] = support['response'].apply(clean_text)

support.shape

# Splitting the dataset into train, validation & test
from sklearn.model_selection import train_test_split
def split_dataset(df):
  train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)
  val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)
  return train_df, val_df, test_df

sentiment_train, sentiment_val, sentiment_test = split_dataset(sentiment)
emotion_train, emotion_val, emotion_test = split_dataset(emotion)
dailydialog_train, dailydialog_val, dailydialog_test = split_dataset(dailydialog)
support_train, support_val, support_test = split_dataset(support)

# Formatting the dataset to T5 format & adding prompts
# Sentiment analysis
sentiment_train['input_text'] = "sentiment: " + sentiment['text']
sentiment_train['target_text'] = sentiment['sentiment']
sentiment_test['input_text'] = "sentiment: " + sentiment['text']
sentiment_test['target_text'] = sentiment['sentiment']
sentiment_val['input_text'] = "sentiment: " + sentiment['text']
sentiment_val['target_text'] = sentiment['sentiment']

# Use the remove_columns method from Hugging Face Dataset
sentiment_train.drop(columns=['text','category', 'rating', 'sentiment'], inplace=True)
sentiment_test.drop(columns=['text','category', 'rating', 'sentiment'], inplace=True)
sentiment_val.drop(columns=['text','category', 'rating', 'sentiment'], inplace=True)

sentiment_test



# Formatting the dataset to T5 format & adding prompts
# Emotion Detection
import random
emotion_prompts= ['Identify the emotion expressed:',
                  'What emotion best describes this text?',
                  'Classify the emotion:',
                  'Emotion label:']
def emotion_preprocess(example):
  prompt = random.choice(emotion_prompts)
  example['input_text'] = f"{prompt} {example['text']}"
  return example

emotion_train = emotion_train.apply(emotion_preprocess, axis=1)

# For test
fixed_prompt = 'Identify the emotion expressed:'
def test_preprocess(example):
  # Use 'input_text' column instead of 'text' if it exists, otherwise use 'text'
  text_column = 'input_text' if 'input_text' in example else 'text'
  example['input_text'] = f"{fixed_prompt} {example[text_column]}"
  return example

emotion_test = emotion_test.apply(test_preprocess, axis=1)
emotion_val = emotion_val.apply(test_preprocess, axis=1)

emotion_train.drop(columns=['text'], inplace=True)
emotion_test.drop(columns=['text'], inplace=True)
emotion_val.drop(columns=['text'], inplace=True)

emotion_train['target_text'] = emotion_train['labels']
emotion_test['target_text'] = emotion_test['labels']
emotion_val['target_text'] = emotion_val['labels']

emotion_train.drop(columns=['labels'], inplace=True)
emotion_test.drop(columns=['labels'], inplace=True)
emotion_val.drop(columns=['labels'], inplace=True)

emotion_train.head()

def dialog_preprocess(example):
  prompt = random.choice(emotion_prompts)
  example['input_text'] = f"{prompt} {example['dialog']}"
  return example

# For test
fixed_prompt = 'Identify the emotion expressed:'
def test_preprocess(example):
  example['input_text'] = f"{fixed_prompt} {example['dialog']}"
  return example

dailydialog_train = dailydialog_train.apply(dialog_preprocess, axis=1)
dailydialog_test = dailydialog_test.apply(test_preprocess, axis=1)
dailydialog_val = dailydialog_val.apply(test_preprocess, axis=1)

dailydialog_test.head()

dailydialog_train.drop(columns=['dialog'], inplace=True)
dailydialog_test.drop(columns=['dialog'], inplace=True)
dailydialog_val.drop(columns=['dialog'], inplace=True)

dailydialog_train['target_text'] = dailydialog_train['emotion_label']
dailydialog_test['target_text'] = dailydialog_test['emotion_label']
dailydialog_val['target_text'] = dailydialog_val['emotion_label']

dailydialog_train.drop(columns=['emotion_label'], inplace=True)
dailydialog_test.drop(columns=['emotion_label'], inplace=True)
dailydialog_val.drop(columns=['emotion_label'], inplace=True)

# Formatting the dataset to T5 format & adding prompts
# Text generation
text_gen_prompts = ["Question: ",
                    "User's query: ",
                    "Customer's inquiry: ",
                    "Query: "]
def random_train_prompt(example):
  prompt = random.choice(text_gen_prompts)
  example['input_text'] = f"{prompt} {example['question']}"
  return example

fixed_prompt = "Question: "
def random_test_prompt(example):
  example['input_text'] = f"{fixed_prompt} {example['question']}"
  return example

support_train = support_train.apply(random_train_prompt, axis=1)
support_test = support_test.apply(random_test_prompt, axis=1)
support_val = support_val.apply(random_test_prompt, axis=1)

support_train.head()

support_train['target_text'] = support_train['response']
support_test['target_text'] = support_test['response']
support_val['target_text'] = support_val['response']

support_train.drop(columns=['question', 'response'], inplace=True)
support_test.drop(columns=['question', 'response'], inplace=True)
support_val.drop(columns=['question', 'response'], inplace=True)

sentiment[["input_text", "target_text"]].to_csv("sentiment_t5.csv", index=False)
emotion[["input_text", "target_text"]].to_csv("goemotions_t5.csv", index=False)
dailydialog[["input_text", "target_text"]].to_csv("dailydialog_t5.csv", index=False)
support[["input_text", "target_text"]].to_csv("support_t5.csv", index=False)

"""# MOUNT GOOGLE DRIVE AND START HERE

**IMPORT MODELS AND LIBRARIES**
"""

!pip install datasets

# Run this first
import requests, gzip, json, pandas as pd
from io import BytesIO
import gc
import numpy as np
import os
from sklearn.model_selection import train_test_split
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from datasets import Dataset
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

from google.colab import drive
drive.mount('/content/drive')

sentiment = pd.read_csv("/content/dailydialog.csv")
emotion = pd.read_csv("/content/goemotions.csv")
dailydialog = pd.read_csv("/content/dailydialog.csv")
support = pd.read_csv("/content/cust_support.csv")

"""TOKENIZING THE DATASETS"""

tokenizer = T5Tokenizer.from_pretrained("t5-small")

# Transforming dataset into hugging face datasets
def transform_data(df):
  df = Dataset.from_pandas(df)
  return df

sentiment_train = transform_data(sentiment_train)
sentiment_val = transform_data(sentiment_val)
sentiment_test = transform_data(sentiment_test)

emotion_train = transform_data(emotion_train)
emotion_val = transform_data(emotion_val)
emotion_test = transform_data(emotion_test)

dailydialog_train = transform_data(dailydialog_train)
dailydialog_val = transform_data(dailydialog_val)
dailydialog_test = transform_data(dailydialog_test)

support_train = transform_data(support_train)
support_val = transform_data(support_val)
support_test = transform_data(support_test)

def tokenize_function(text):
    model_inputs = tokenizer(
        text["input_text"],
        max_length=512,
        padding="max_length",
        truncation=True,
    )

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            text["target_text"],
            max_length=128,
            padding="max_length",
            truncation=True,
        )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Apply the Tokenization function to the datasets
sentiment_train = sentiment_train.map(tokenize_function, batched=True)
sentiment_val = sentiment_val.map(tokenize_function, batched=True)
sentiment_test = sentiment_test.map(tokenize_function, batched=True)

emotion_train = emotion_train.map(tokenize_function, batched=True)
emotion_val = emotion_val.map(tokenize_function, batched=True)
emotion_test = emotion_test.map(tokenize_function, batched=True)

dailydialog_train = dailydialog_train.map(tokenize_function, batched=True)
dailydialog_val = dailydialog_val.map(tokenize_function, batched=True)
dailydialog_test = dailydialog_test.map(tokenize_function, batched=True)

support_train = support_train.map(tokenize_function, batched=True)
support_val = support_val.map(tokenize_function, batched=True)
support_test = support_test.map(tokenize_function, batched=True)

import torch
print(torch.cuda.is_available())  # Should return 'True' if GPU is available

"""# SENTIMENT ANALYSIS"""

model = T5ForConditionalGeneration.from_pretrained("t5-small")

training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/T5_Chatbot_Project/outputs/sentiment_analysis",
    run_name= "t5-sentiment-run1",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=2,
    dataloader_drop_last=False,
    num_train_epochs=4,
    weight_decay=0.01,
    save_total_limit=2,
    logging_dir="/content/drive/MyDrive/T5_Chatbot_Project/logs/sentiment_analysis",
    load_best_model_at_end=True,
    fp16=False
)

sentiment_trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=sentiment_train,    # tokenized train set
    eval_dataset= sentiment_val,       # tokenized validation set
    tokenizer=tokenizer
)

sentiment_trainer.train() # resume_from_checkpoint=True)
sentiment_trainer.save_model("/content/drive/MyDrive/T5_Chatbot_Project/models/sentiment_analysis")

import matplotlib.pyplot as plt
training_loss = [0.0044, 0.0036, 0.0035, 0.0033]
validation_loss = [0.00345, 0.00316, 0.003005, 0.003011]
epoch = [1, 2, 3, 4]

plt.plot(epoch, training_loss, label='Training Loss')
plt.plot(epoch, validation_loss, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

from tensorboard.backend.event_processing.event_accumulator import EventAccumulator
import matplotlib.pyplot as plt

# Load the event data
event_acc = EventAccumulator('/content/drive/MyDrive/T5_Chatbot_Project/logs/sentiment_analysis')
event_acc.Reload()

# Get scalar metrics (e.g., loss)
train_loss = event_acc.Scalars('train/loss')
eval_loss = event_acc.Scalars('eval/loss')

# Extract values
train_steps = [x.step for x in train_loss]
train_values = [x.value for x in train_loss]

eval_steps = [x.step for x in eval_loss]
eval_values = [x.value for x in eval_loss]

# Plotting
plt.figure(figsize=(10,5))
plt.plot(train_steps, train_values, label='Training Loss')
plt.plot(eval_steps, eval_values, label='Validation Loss')

# Adjust the y-axis for better visibility
plt.yscale('log')  # Use logarithmic scale if losses span orders of magnitude

# If you don't want log scale, you can also limit the y-axis to focus on relevant losses
# plt.ylim([0, 1])  # Example of limiting y-axis from 0 to 1 for better focus

plt.xlabel("Steps")
plt.ylabel("Loss")
plt.title("Training and Validation Loss Over Steps")
plt.legend()
plt.grid(True)
plt.show()

model = T5ForConditionalGeneration.from_pretrained("/content/drive/MyDrive/T5_Chatbot_Project/models/sentiment_analysis")

torch.cuda.empty_cache()

# Access the event data using the correct path
event_acc = EventAccumulator('/content/drive/MyDrive/T5_Chatbot_Project/logs/sentiment_analysis')
event_acc.Reload()

# Get available tags (replace with the correct path to your logs)
available_tags = event_acc.Tags()

# Check if "loss" tag exists
print(f"'loss' tag in TensorBoard events: {'loss' in available_tags.get('scalars', [])}")

# Print all available tags if needed for debugging
print(available_tags)

# Commented out IPython magic to ensure Python compatibility.
# Access the logs from trainer after training
# Load TensorBoard in Colab (or use a Jupyter Notebook)
# %load_ext tensorboard
# %tensorboard --logdir "/content/drive/MyDrive/T5_Chatbot_Project/logs/sentiment_analysis"

"""MODEL PREDICTION & EVALUATION"""

# DON'T RUN THIS CELL
model.eval()

chunk_size = 100
all_preds, all_refs = [], []

# Loop over test set in chunks
for i in range(0, len(sentiment_test), chunk_size):
    print(f"Processing chunk {i} to {i+chunk_size}")
    chunk = sentiment_test.select(range(i, min(i + chunk_size, len(sentiment_test))))

    output = trainer.predict(chunk)

    preds = tokenizer.batch_decode(output.predictions, skip_special_tokens=True)
    refs = tokenizer.batch_decode(output.label_ids, skip_special_tokens=True)

    all_preds.extend([p.strip() for p in preds])
    all_refs.extend([r.strip() for r in refs])

os.makedirs("/content/drive/MyDrive/T5_Chatbot_Project/results", exist_ok=True)

from transformers import default_data_collator
from torch.utils.data import DataLoader
import torch

model.eval()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# DataLoader with proper collate
dataloader = DataLoader(
    sentiment_test,
    batch_size=8,
    collate_fn=default_data_collator
)

all_preds = []
all_refs = []

for i, batch in enumerate(dataloader):
    input_ids = batch['input_ids'].to(device)
    attention_mask = batch['attention_mask'].to(device)
    labels = batch['labels'].to(device)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=32
        )
    preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    refs = tokenizer.batch_decode(batch['labels'], skip_special_tokens=True)

    all_preds.extend(preds)
    all_refs.extend(refs)
    if i % 10 == 0:
        print(f"Processed batch {i + 1} of ~{len(dataloader)}")

print("Accuracy:", accuracy_score(all_refs, all_preds))
print("F1 Score:", f1_score(all_refs, all_preds, average="weighted"))
print("Precision:", precision_score(all_refs, all_preds, average="weighted"))
print("Recall:", recall_score(all_refs, all_preds, average="weighted"))

results_df = pd.DataFrame({
    "predicted": all_preds,
    "actual": all_refs
})

import os

# Before saving the DataFrame:
os.makedirs("/content/drive/MyDrive/T5_Chatbot_Project/results/sentiment_analysis", exist_ok=True)

# Now you can save:
results_df.to_csv("/content/drive/MyDrive/T5_Chatbot_Project/results/sentiment_analysis/predictions.csv", index=False)

with open("/content/drive/MyDrive/T5_Chatbot_Project/results/sentiment_analysis/perf_metrics.txt", "w") as f:
    f.write('Results for Sentiment Analysis:\n')
    f.write(f"Accuracy: {accuracy_score(all_refs, all_preds):.4f}\n")
    f.write(f"F1 Score: {f1_score(all_refs, all_preds, average='weighted'):.4f}\n")
    f.write(f"Precision: {precision_score(all_refs, all_preds, average='weighted'):.4f}\n")
    f.write(f"Recall: {recall_score(all_refs, all_preds, average='weighted'):.4f}\n")

"""# EMOTION DETECTION"""

model = T5ForConditionalGeneration.from_pretrained("t5-small")

training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/T5_Chatbot_Project/models/t5_emotion_detection",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=2,
    eval_strategy="epoch",
    num_train_epochs=10,
    save_strategy="epoch",
    logging_dir="/content/drive/MyDrive/T5_Chatbot_Project/logs/emotion_detection",
    logging_steps=10,
    save_total_limit=1,
    load_best_model_at_end=True,
)

emotion_trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=emotion_train,
    eval_dataset=emotion_val,
    tokenizer=tokenizer
)

model=T5ForConditionalGeneration.from_pretrained("/content/drive/MyDrive/T5_Chatbot_Project/models/t5_emotion_detection/checkpoint-23740")
emotion_trainer.train(resume_from_checkpoint=True)
emotion_trainer.save_model("/content/drive/MyDrive/T5_Chatbot_Project/models/t5_emotion_detection")

emotion_trainer.save_model("/content/drive/MyDrive/T5_Chatbot_Project/models/t5_emotion_detection")

model = T5ForConditionalGeneration.from_pretrained("/content/drive/MyDrive/T5_Chatbot_Project/models/t5_emotion_detection")

# Create NEW arguments
new_args = TrainingArguments(
    dataloader_drop_last=False,
    fp16=False,
)

# Reconnect Trainer with updated args
emotion_trainer = Trainer(
    model=model,
    args=new_args,
    tokenizer=tokenizer
)

from tensorboard.backend.event_processing.event_accumulator import EventAccumulator
import matplotlib.pyplot as plt

# Load the event data
event_acc = EventAccumulator('/content/drive/MyDrive/T5_Chatbot_Project/logs/emotion_detection')
event_acc.Reload()

# Get scalar metrics (e.g., loss)
train_loss = event_acc.Scalars('train/loss')
eval_loss = event_acc.Scalars('eval/loss')

# Extract values
train_steps = [x.step for x in train_loss]
train_values = [x.value for x in train_loss]

eval_steps = [x.step for x in eval_loss]
eval_values = [x.value for x in eval_loss]

# Plotting
plt.figure(figsize=(10,5))
plt.plot(train_steps, train_values, label='Training Loss')
plt.plot(eval_steps, eval_values, label='Validation Loss')

# Adjust the y-axis for better visibility
# plt.yscale('log')  # Use logarithmic scale if losses span orders of magnitude

# If you don't want log scale, you can also limit the y-axis to focus on relevant losses
# plt.ylim([0, 1])  # Example of limiting y-axis from 0 to 1 for better focus

plt.xlabel("Steps")
plt.ylabel("Loss")
plt.title("Training and Validation Loss for Emotion Detection over Steps")
plt.savefig('emotion_loss_plot.png', dpi=300)
plt.legend()
plt.grid(True)
plt.show()

"""MODEL PREDICTION & EVALUATION"""

from transformers import default_data_collator
from torch.utils.data import DataLoader

model.eval()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# DataLoader with proper collate
dataloader = DataLoader(
    emotion_test,
    batch_size=8,
    collate_fn=default_data_collator
)

all_preds = []
all_refs = []

for i, batch in enumerate(dataloader):
    input_ids = batch['input_ids'].to(device)
    attention_mask = batch['attention_mask'].to(device)
    labels = batch['labels'].to(device)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=32
        )
    preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    refs = tokenizer.batch_decode(batch['labels'], skip_special_tokens=True)

    all_preds.extend(preds)
    all_refs.extend(refs)
    if i % 10 == 0:
        print(f"Processed batch {i + 1} of {len(dataloader)}")

print("Accuracy:", accuracy_score(all_refs, all_preds))
print("F1 Score:", f1_score(all_refs, all_preds, average="weighted"))
print("Precision:", precision_score(all_refs, all_preds, average="weighted"))
print("Recall:", recall_score(all_refs, all_preds, average="weighted"))

results_df = pd.DataFrame({
    "predicted": all_preds,
    "actual": all_refs
})
results_df.to_csv("/content/drive/MyDrive/T5_Chatbot_Project/results/emotion_predictions.csv", index=False)

with open("/content/drive/MyDrive/T5_Chatbot_Project/results/emotion_detection.txt", "w") as f:
    f.write('Results for Emotion Detection (GoEmotions):\n')
    f.write(f"Accuracy: {accuracy_score(all_refs, all_preds):.4f}\n")
    f.write(f"F1 Score: {f1_score(all_refs, all_preds, average='weighted'):.4f}\n")
    f.write(f"Precision: {precision_score(all_refs, all_preds, average='weighted'):.4f}\n")
    f.write(f"Recall: {recall_score(all_refs, all_preds, average='weighted'):.4f}\n")

"""DAILYDIALOG"""

model = T5ForConditionalGeneration.from_pretrained("t5-small")
training_args = TrainingArguments(
    output_dir="/content/drive/MyDrive/T5_Chatbot_Project/models/t5_dailydialog",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=2,
    eval_strategy="epoch",
    num_train_epochs=5,
    save_strategy="epoch",
    logging_dir="/content/drive/MyDrive/T5_Chatbot_Project/logs/dailydialog",
    logging_steps=10,
    save_total_limit=1,
    dataloader_drop_last=False,
    fp16=False,
    load_best_model_at_end=True,
)

dailydialog_trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dailydialog_train,
    eval_dataset=dailydialog_val,
    tokenizer=tokenizer
)

dailydialog_trainer.train()
dailydialog_trainer.save_model("/content/drive/MyDrive/T5_Chatbot_Project/models/t5_dailydialog")

from tensorboard.backend.event_processing.event_accumulator import EventAccumulator
import matplotlib.pyplot as plt

# Load the event data
event_acc = EventAccumulator('/content/drive/MyDrive/T5_Chatbot_Project/logs/dailydialog')
event_acc.Reload()

# Get scalar metrics (e.g., loss)
train_loss = event_acc.Scalars('train/loss')
eval_loss = event_acc.Scalars('eval/loss')

# Extract values
train_steps = [x.step for x in train_loss]
train_values = [x.value for x in train_loss]

eval_steps = [x.step for x in eval_loss]
eval_values = [x.value for x in eval_loss]

# Plotting
plt.figure(figsize=(10,5))
plt.plot(train_steps, train_values, label='Training Loss')
plt.plot(eval_steps, eval_values, label='Validation Loss')

# Adjust the y-axis for better visibility
plt.yscale('log')  # Use logarithmic scale if losses span orders of magnitude

# If you don't want log scale, you can also limit the y-axis to focus on relevant losses
# plt.ylim([0, 1])  # Example of limiting y-axis from 0 to 1 for better focus

plt.xlabel("Steps")
plt.ylabel("Loss")
plt.title("Training and Validation Loss for Emotion Detection over Steps")
plt.savefig('dailydialog_loss_plot.png', dpi=300)
plt.legend()
plt.grid(True)
plt.show()

model = T5ForConditionalGeneration.from_pretrained("/content/drive/MyDrive/T5_Chatbot_Project/models/t5_dailydialog")

"""MODEL PREDICTION & EVALUATION"""

model.eval()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# DataLoader with proper collate
dataloader = DataLoader(
    dailydialog_test,
    batch_size=8,
    collate_fn=default_data_collator
)

all_preds = []
all_refs = []

for i, batch in enumerate(dataloader):
    input_ids = batch['input_ids'].to(device)
    attention_mask = batch['attention_mask'].to(device)
    labels = batch['labels'].to(device)

    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=32
        )
    preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    refs = tokenizer.batch_decode(batch['labels'], skip_special_tokens=True)

    all_preds.extend(preds)
    all_refs.extend(refs)
    if i % 10 == 0:
        print(f"Processed batch {i + 1} of {len(dataloader)}")

print("Accuracy:", accuracy_score(all_refs, all_preds))
print("F1 Score:", f1_score(all_refs, all_preds, average="weighted"))
print("Precision:", precision_score(all_refs, all_preds, average="weighted"))
print("Recall:", recall_score(all_refs, all_preds, average="weighted"))

results_df = pd.DataFrame({
    "predicted": all_preds,
    "actual": all_refs
})
results_df.to_csv("/content/drive/MyDrive/T5_Chatbot_Project/results/dailydialog.csv", index=False)

with open("/content/drive/MyDrive/T5_Chatbot_Project/results/dailydialog_metrics.txt", "w") as f:
    f.write('Results for Emotion Detection (DailyDialog):\n')
    f.write(f"Accuracy: {accuracy_score(all_refs, all_preds):.4f}\n")
    f.write(f"F1 Score: {f1_score(all_refs, all_preds, average='weighted'):.4f}\n")
    f.write(f"Precision: {precision_score(all_refs, all_preds, average='weighted'):.4f}\n")
    f.write(f"Recall: {recall_score(all_refs, all_preds, average='weighted'):.4f}\n")

"""# TEXT GENERATION"""

from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
import torch

import pandas as pd
cust_support = pd.read_csv("/content/cust_support.csv")
support = cust_support[['question', 'response']].dropna().head(30000)

import re

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r"http\S+", "", text)  # remove URLs
    text = re.sub(r"<.*?>", "", text)    # remove HTML tags
    text = re.sub(r"[^a-zA-Z\s.,!?]", "", text)
    text = re.sub(r"\s+", " ", text).strip()  # normalize spaces
    return text

support['question'] = support['question'].apply(clean_text)
support['response'] = support['response'].apply(clean_text)

tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-medium", padding_side="left")

# Combine input and output into one sequence (input --> target)
support["dialogue"] = 'User: ' + support["question"] + tokenizer.eos_token + 'Bot:' + support["response"] + tokenizer.eos_token

support.head()

from sklearn.model_selection import train_test_split
from datasets import Dataset
support_train, temp_df = train_test_split(support, test_size=0.2, random_state=16)
support_val, support_test = train_test_split(temp_df, test_size=0.33, random_state=16)

support_test.head()

support_train = Dataset.from_pandas(support_train)
support_val = Dataset.from_pandas(support_val)
support_test = Dataset.from_pandas(support_test)

from datasets import Dataset
def tokenize_function(examples):
    tokenizer.pad_token = tokenizer.eos_token
    encodings = tokenizer(examples["dialogue"], truncation=True, padding="max_length", max_length=512)
    encodings["labels"] = encodings["input_ids"].copy()
    return encodings

support_train = support_train.map(tokenize_function, batched=True)
support_val = support_val.map(tokenize_function, batched=True)
support_test = support_test.map(tokenize_function, batched=True)

torch.cuda.empty_cache()

model = AutoModelForCausalLM.from_pretrained("microsoft/DialoGPT-medium")

training_args = TrainingArguments(
    run_name="dialogpt",
    output_dir="/content/drive/MyDrive/T5_Chatbot_Project/outputs/dialogpt_medium",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=2,
    learning_rate=5e-5,
    warmup_steps=500,
    weight_decay=0.01,
    eval_strategy="epoch",
    logging_dir="/content/drive/MyDrive/T5_Chatbot_Project/logs/dialogpt_medium",
    logging_steps=10,
    num_train_epochs=3,
    save_strategy="epoch",
    save_total_limit=2,
    dataloader_drop_last=True,
    fp16=True,
    load_best_model_at_end=True,
    gradient_accumulation_steps=2,
    seed=16
)

dialogpt_medium_trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=support_train,
    eval_dataset=support_val,
    tokenizer=tokenizer
)

import torch
torch.cuda.is_available()

dialogpt_medium_trainer.train()
dialogpt_medium_trainer.save_model("/content/drive/MyDrive/T5_Chatbot_Project/models/dialogpt_medium")

from tensorboard.backend.event_processing.event_accumulator import EventAccumulator
import matplotlib.pyplot as plt

# Load the event data
event_acc = EventAccumulator('/content/drive/MyDrive/T5_Chatbot_Project/logs/dialogpt_medium')
event_acc.Reload()

# Get scalar metrics (e.g., loss)
train_loss = event_acc.Scalars('train/loss')
eval_loss = event_acc.Scalars('eval/loss')

# Extract values
train_steps = [x.step for x in train_loss]
train_values = [x.value for x in train_loss]

eval_steps = [x.step for x in eval_loss]
eval_values = [x.value for x in eval_loss]

# Plotting
plt.figure(figsize=(10,5))
plt.plot(train_steps, train_values, label='Training Loss')
plt.plot(eval_steps, eval_values, label='Validation Loss')


# Adjust the y-axis for better visibility
plt.yscale('log')  # Use logarithmic scale if losses span orders of magnitude

# If you don't want log scale, you can also limit the y-axis to focus on relevant losses
# plt.ylim([0, 1])  # Example of limiting y-axis from 0 to 1 for better focus

plt.xlabel("Steps")
plt.ylabel("Loss")
plt.title("Training and Validation Loss Over Step")
plt.legend()
plt.savefig('dialogpt_loss_plot.png', dpi=300)
plt.grid(True)
plt.show()

model = AutoModelForCausalLM.from_pretrained("/content/drive/MyDrive/T5_Chatbot_Project/models/dialogpt_medium")
tokenizer = AutoTokenizer.from_pretrained("/content/drive/MyDrive/T5_Chatbot_Project/models/dialogpt_medium")

"""MODEL PREDICTION AND EVALUATION"""

!pip install evaluate
!pip install rouge_score

from torch.utils.data import DataLoader
import torch

# Initialize list to store predictions
all_preds = []

# Chat function to take input from dataset and generate a response
def chat(prompt):
    input_ids = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors='pt')

    # Generate response
    output_ids = model.generate(
        input_ids,
        max_length=100,
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        temperature=0.7
    )

    # Decode the generated output and return as string
    reply = tokenizer.decode(output_ids[:, input_ids.shape[-1]:][0], skip_special_tokens=True)
    return reply

# Prepare DataLoader for dataset
dataloader = DataLoader(support_test, batch_size=8)
model.eval()

# Iterate over batches in test dataset
for batch in dataloader:
    prompts = batch['question']

    # Generate responses for each prompt
    for prompt in prompts:
        response = chat(prompt)
        all_preds.append(response)
        print(f"Prompt: {prompt}\nResponse: {response}\n")
# Output the first 5 predictions for verification
print(all_preds[:5])

all_refs = []

# Extract true responses (target text) from the dataset
target = support_test['response'][:len(all_preds)]
all_refs.extend(target)

print(f"Sample of true responses (all_refs): {all_refs[:5]}")

# Evaluate
import evaluate
metric_bleu = evaluate.load("bleu")
metric_rouge = evaluate.load("rouge")

results_bleu = metric_bleu.compute(predictions= all_preds, references=[[r] for r in all_refs])
results_rouge = metric_rouge.compute(predictions=all_preds, references=all_refs)

print("BLEU:", results_bleu["bleu"])
print("ROUGE-1:", results_rouge["rouge1"])
print("ROUGE-2:", results_rouge["rouge2"])
print("ROUGE-L:", results_rouge["rougeL"])

import os

os.makedirs("/content/drive/MyDrive/T5_Chatbot_Project/results/dialogpt_medium", exist_ok=True)

with open("/content/drive/MyDrive/T5_Chatbot_Project/results/dialogpt_medium/resultmetrics.txt", "w") as f:
    f.write('Results for Text generation:\n')
    f.write(f"BLEU: {results_bleu['bleu']}\n")
    f.write(f"ROUGE-1: {results_rouge['rouge1']}\n")
    f.write(f"ROUGE-2: {results_rouge['rouge2']}\n")
    f.write(f"ROUGE-L: {results_rouge['rougeL']}\n")

"""# FULL AUTOMATION PROCESS"""

from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoModelForCausalLM, AutoTokenizer
import torch

# Load sentiment T5 model
sentiment_tokenizer = T5Tokenizer.from_pretrained("/content/drive/MyDrive/T5_Chatbot_Project/models/sentiment_analysis")
sentiment = T5ForConditionalGeneration.from_pretrained("/content/drive/MyDrive/T5_Chatbot_Project/models/sentiment_analysis")

# Load emotion T5 model
emotion_tokenizer = T5Tokenizer.from_pretrained("/content/drive/MyDrive/T5_Chatbot_Project/models/t5_emotion_detection")
emotion = T5ForConditionalGeneration.from_pretrained("/content/drive/MyDrive/T5_Chatbot_Project/models/t5_emotion_detection")

# Load DialoGPT model
dialogpt_tokenizer = AutoTokenizer.from_pretrained("/content/drive/MyDrive/T5_Chatbot_Project/models/dialogpt_medium")
dialogpt_medium = AutoModelForCausalLM.from_pretrained("/content/drive/MyDrive/T5_Chatbot_Project/models/dialogpt_medium")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
sentiment.to(device)
emotion.to(device)
dialogpt_medium.to(device)
# Inference functions
def classify_sentiment(text):
    prompt = f"sentiment: {text}"
    inputs = sentiment_tokenizer(prompt, return_tensors="pt").to(device)
    outputs = sentiment.generate(**inputs)
    return sentiment_tokenizer.decode(outputs[0], skip_special_tokens=True)

def classify_emotion(text):
    inputs = emotion_tokenizer(text, return_tensors="pt").to(device)
    outputs = emotion.generate(**inputs)
    return emotion_tokenizer.decode(outputs[0], skip_special_tokens=True)

def generate_response(user_input, sentiment, emotion):
    prompt = f"User: {user_input} [Sentiment: {sentiment}] [Emotion: {emotion}] {dialogpt_tokenizer.eos_token}"
    inputs = dialogpt_tokenizer.encode(prompt, return_tensors="pt").to(device)
    output = dialogpt_medium.generate(
        inputs,
        max_new_tokens=100,
        pad_token_id=dialogpt_tokenizer.eos_token_id,
        do_sample=True,
        top_p=0.9,
        temperature=0.8
    )
    response = dialogpt_tokenizer.decode(output[:, inputs.shape[-1]:][0], skip_special_tokens=True)
    return response

# Main pipeline
def chatbot_pipeline(user_input):
    sentiment = classify_sentiment(user_input)
    emotion = classify_emotion(user_input)
    print(f"Sentiment: {sentiment}, Emotion: {emotion}")
    response = generate_response(user_input, sentiment, emotion)
    return response

chatbot_pipeline('I am unhappy with my order, the product is damaged')